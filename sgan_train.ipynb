{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to use wandb for visualization you can install it in your workbook as show below\n",
    "!pip install wandb -qU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log in to your W&B account\n",
    "import wandb\n",
    "wandb.login(key='') # add your key here to log in to wandb if you want to use it for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as md\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.utils as vutils\n",
    "import csv\n",
    "import gc\n",
    "import math\n",
    "from torchsummary import summary\n",
    "from datetime import datetime\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inputs\n",
    "# Size of z latent vector (batch_idx.e. size of generator input)\n",
    "nz = 512\n",
    "\n",
    "# Size of window for appliance samples, batch size and max epochs declaration\n",
    "window_size = 1500 #4319  #21599 # 6 hour windows\n",
    "batch_size = 8\n",
    "max_epochs = 200\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "\n",
    "# Parameters\n",
    "params = {'batch_size': batch_size,\n",
    "          'shuffle': True,\n",
    "            'num_workers': 2,\n",
    "            'drop_last': True} # using drop_last = True so that samples returned are divisible by batch size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our custom dataloader used to load the data into the GAN architecture\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    \n",
    "\n",
    "    def __init__(self, df, window_size):\n",
    "        'Initialization'\n",
    "        # convert to numpy and create tensors from the data\n",
    "        x = df.iloc[:, :].to_numpy()\n",
    "        self.window_size = window_size\n",
    "\n",
    "        self.x_train = torch.tensor(list(x))\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.x_train)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        return self.x_train[index].reshape(1, self.window_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Generator and Discriminator architectures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrintLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PrintLayer, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Print for debugging purposes\n",
    "        print(x)\n",
    "        print(x.size())\n",
    "        return x\n",
    "\n",
    "class Generator(nn.Module):\n",
    "\n",
    "    def __init__(self, input, in_channels, out_channels, kernel_size,kernel_size2, output, groups=1):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        left, right = kernel_size // 2, kernel_size // 2\n",
    "        if kernel_size % 2 == 0:\n",
    "            right -= 1\n",
    "        padding = (left, right, 0, 0)\n",
    "\n",
    "        interm = input-2*(kernel_size -1) - 1*(kernel_size2 -1)\n",
    "        interm_2 = (interm - 1)*2 + kernel_size\n",
    "        interm_3 = (interm_2 -1)*2 + kernel_size\n",
    "        interm_4 = (interm_3 -1)*2 + kernel_size\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, 16, kernel_size, groups=groups),\n",
    "            nn.LeakyReLU(0.1, inplace=False),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Conv1d(16, 16, kernel_size2, groups=groups),\n",
    "            nn.LeakyReLU(0.1, inplace=False),\n",
    "            nn.Conv1d(16, 4, kernel_size, groups=groups),\n",
    "            nn.BatchNorm1d(4),\n",
    "            nn.LeakyReLU(0.1, inplace=False),\n",
    "        )\n",
    "\n",
    "        self.deconv = nn.Sequential(\n",
    "            nn.ConvTranspose1d(4, 2, kernel_size, stride=2, groups=groups),\n",
    "            nn.LeakyReLU(0.1, inplace=False),\n",
    "            nn.ConvTranspose1d(2, 2, kernel_size, stride=2, groups=groups),\n",
    "            nn.LeakyReLU(0.1, inplace=False),\n",
    "            nn.ConvTranspose1d(2, 1, kernel_size, stride=2, groups=groups),\n",
    "            nn.LeakyReLU(0.1, inplace=False),\n",
    "        )\n",
    "\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(interm_4, interm_4*2,bias=True),\n",
    "            nn.BatchNorm1d(interm_4*2), #for more stability\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "        )\n",
    "\n",
    "        self.outlayer = nn.Sequential(\n",
    "\n",
    "           nn.Linear( interm_4*2, output,bias=True),\n",
    "\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.deconv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.linear(x)\n",
    "        x = self.outlayer(x)\n",
    "        x = nn.Tanh()(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# Discriminator Code\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input, in_channels, kernel_size, groups ):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        left, right = kernel_size // 2, kernel_size // 2\n",
    "        if kernel_size % 2 == 0:\n",
    "            right -= 1\n",
    "        padding = (left, right, 0, 0)\n",
    "\n",
    "        interm = input-(kernel_size-1)-1*(9-1)\n",
    "        interm_2 =math.ceil((interm - 14)/2)\n",
    "        interm_4 = math.ceil((interm_2 - 8)/2)\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, out_channels=16, kernel_size=kernel_size, groups=groups, padding=0),\n",
    "            nn.LeakyReLU(0.1, inplace=False),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Conv1d(16, 8, kernel_size=9, groups=groups, padding=0),\n",
    "            nn.LeakyReLU(0.1, inplace=False),\n",
    "        )\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "\n",
    "            nn.Conv1d(8, 3, kernel_size=15,stride=2, groups=groups, padding=0),\n",
    "            nn.LeakyReLU(0.1, inplace=False),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Conv1d(3, 1, kernel_size=9, stride=2, groups=groups, padding=0),\n",
    "            nn.LeakyReLU(0.1, inplace=False)\n",
    "\n",
    "        )\n",
    "\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(interm_4 , round(interm_4/2),bias=True),  #fully connected layer\n",
    "            nn.LeakyReLU(0.05, inplace=True),\n",
    "            nn.Linear(round(interm_4/2), 1,bias=True),  #fully connected layer\n",
    "            nn.Sigmoid()\n",
    "\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.main(x)\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.linear(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom weights initialization called on netG and netD\n",
    "# Apply the weights_init function to randomly initialize all weights\n",
    "#  to mean=0, stdev=0.02.\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "\n",
    "\n",
    "# create latent noise from gaussian distribution\n",
    "def noise_samples(batch_size, nz ):\n",
    "    fixed_noise = (torch.randn(batch_size, 1, nz, device=device) - 0.5) / 0.5\n",
    "    #fixed_noise = torch.rand(batch_size, 1, nz, device=device)\n",
    "    #fixed_noise = torch.normal(0.5, 1, size=(batch_size, 1, nz), device=device)\n",
    "    #fixed_noise = torch.zeros(batch_size, 1, nz, device=device)\n",
    "    return fixed_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the final and cleaned dataset for the appliance\n",
    "train_pre = pd.read_pickle('path_to_saved/dataport_electric_vehicle_5s_clean.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data into the dataloader\n",
    "training_set = Dataset(train_pre, window_size)\n",
    "training_dataloader = torch.utils.data.DataLoader(training_set, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize the discriminator network\n",
    "netD = Discriminator(input=window_size, in_channels=1, kernel_size=25, groups=1).to(device)\n",
    "if torch.cuda.is_available():\n",
    "  netD.cuda()\n",
    "netD.apply(weights_init)\n",
    "print(netD)\n",
    "\n",
    "# initialize the generator network\n",
    "netG = Generator(input= nz, in_channels=1, out_channels=1, kernel_size=25, kernel_size2=75, output=window_size).to(device)\n",
    "if torch.cuda.is_available():\n",
    "  netG.cuda()\n",
    "\n",
    "# Apply the weights_init function to randomly initialize all weights\n",
    "#  to mean=0, stdev=0.02.\n",
    "netG.apply(weights_init)\n",
    "# Print the model\n",
    "print(netG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED=42 #42\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main train loop!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "  torch.cuda.empty_cache()\n",
    "  adversarial_loss = torch.nn.BCELoss()\n",
    "  # Initialize the loss function\n",
    "  kl_loss = nn.KLDivLoss(reduction=\"batchmean\") \n",
    "  criterion = nn.MSELoss()\n",
    "  softmax = torch.nn.Softmax()\n",
    "  fixed_noise = noise_samples(batch_size, nz )\n",
    "\n",
    "  # Establish convention for real and fake labels during training - label smoothing\n",
    "  real_label = 0.9\n",
    "  fake_label = 0  # could also try 0.1\n",
    "\n",
    "  # Learning rate for optimizers\n",
    "  lr_d = 0.00004\n",
    "  lr_g = 0.00007\n",
    "\n",
    "  now = datetime.now()\n",
    "\n",
    "  # if you use wandb, the below code can create dashboards for review of the experiments, if not, \n",
    "  # you should delete this\n",
    "  wandb.init(\n",
    "    # Set the project where this run will be logged\n",
    "    project=\"ddgan-nilm\",\n",
    "    name=f\"experiment_1 \"+ now.strftime(\"%m/%d/%Y, %H:%M:%S\"),   # add timestamp to name \n",
    "    # Track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"lr_d\": lr_d,\n",
    "    \"lr_g\": lr_g,\n",
    "    \"batch_size\":batch_size,\n",
    "    \"window_size\":window_size,\n",
    "    \"nz\": nz,\n",
    "    \"loss\": criterion,\n",
    "    \"architecture\": \"DDGAN\",\n",
    "    \"dataset\": \"1s_data_newyork_file\",\n",
    "    \"epochs\": max_epochs,\n",
    "    })\n",
    "\n",
    "  # Setup Adam optimizers for both G and D\n",
    "  optimizerD = optim.Adam(netD.parameters(), lr=lr_d, betas=(0.5, 0.999))\n",
    "  optimizerG = optim.Adam(netG.parameters(), lr=lr_g, betas=(0.5, 0.999))\n",
    "\n",
    "\n",
    "  energ_list = []\n",
    "  G_losses = []\n",
    "  D_losses = []\n",
    "  iters = 0\n",
    "\n",
    "  for epoch in range(max_epochs):\n",
    "      # Training\n",
    "      # save the model every couple of epochs in case training fails or is stopped\n",
    "      print('epoch: ', epoch)\n",
    "      if (epoch % 10 == 0):\n",
    "              torch.save(netD, 'path_to_saved/discriminator_temp_model.pt')\n",
    "              torch.save(netG, 'path_to_saved/generator_temp_model.pt')\n",
    "              print('saved models!')\n",
    "      for batch_idx, data in enumerate(training_dataloader, 0):\n",
    "\n",
    "          #print('batch index: ',batch_idx)\n",
    "          #print(data)\n",
    "\n",
    "          ############################\n",
    "          # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "          ###########################\n",
    "\n",
    "          # Train with all-real batch\n",
    "          optimizerD.zero_grad()\n",
    "\n",
    "          # Format batch\n",
    "          real_cpu = data.to(device)\n",
    "          #print(real_cpu)\n",
    "          #print(real_cpu.shape)\n",
    "\n",
    "          b_size = real_cpu.size(0)\n",
    "          #print('b size: ', b_size)\n",
    "\n",
    "          #create the real labels for the batch\n",
    "          label = torch.full((b_size,), real_label, dtype=torch.float, device=device)\n",
    "          #print('labels: ', label)\n",
    "          #print(label.shape)\n",
    "          #print(real_cpu)\n",
    "          #print(real_cpu.shape)\n",
    "\n",
    "          # Forward pass real batch through D\n",
    "          output = netD(real_cpu).view(-1)\n",
    "          #print('output ',output)\n",
    "          # Calculate loss on full real batch\n",
    "          #print('real output:')\n",
    "          #print(output)\n",
    "          #print('labels:')\n",
    "          #print(label)\n",
    "\n",
    "          errD_real = adversarial_loss(output, label)\n",
    "\n",
    "          # Calculate gradients for D in backward pass\n",
    "          errD_real.backward()\n",
    "          D_x = output.mean().item()\n",
    "\n",
    "          # Train with all-fake batch\n",
    "          # Generate batch of latent vectors\n",
    "          noise = noise_samples(batch_size, nz )\n",
    "\n",
    "          # Generate fake time series batch with G\n",
    "          fake = netG(noise)\n",
    "          #print('fake ', fake)\n",
    "          label.fill_(fake_label)\n",
    "          # Classify all fake batch with D\n",
    "          #print(fake)\n",
    "          fake = fake.reshape(batch_size,1, window_size)\n",
    "          output = netD(fake.detach()).view(-1)\n",
    "          #print('fake output:')\n",
    "          #print(output)\n",
    "          #print('fake labels:')\n",
    "          #print(label)\n",
    "          # Calculate D's loss on the all-fake batch\n",
    "          errD_fake = adversarial_loss(output, label)\n",
    "          # Calculate the gradients for this batch, accumulated (summed) with previous gradients\n",
    "          errD_fake.backward()\n",
    "          D_G_z1 = output.mean().item()\n",
    "          # Compute error of D as sum over the fake and the real batches\n",
    "          errD = errD_real + errD_fake\n",
    "          # Update D\n",
    "          optimizerD.step()\n",
    "\n",
    "          ############################\n",
    "          # (2) Update G network: maximize log(D(G(z)))\n",
    "          ###########################\n",
    "          optimizerG.zero_grad()\n",
    "          label.fill_(real_label)  # fake labels are real for generator cost\n",
    "          # Since we just updated D, perform another forward pass of all-fake batch through D\n",
    "          output = netD(fake).view(-1)\n",
    "          # Calculate G's loss based on this output\n",
    "          errG = criterion(output, label)\n",
    "          # Calculate gradients for G\n",
    "          errG.backward()\n",
    "          D_G_z2 = output.mean().item()\n",
    "          # Update G\n",
    "          optimizerG.step()\n",
    "\n",
    "          # Output training stats\n",
    "          if batch_idx % 50 == 0:\n",
    "              print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
    "                      % (epoch, max_epochs, batch_idx, len(training_dataloader),\n",
    "                         errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "\n",
    "          # Save Losses for plotting later\n",
    "          G_losses.append(errG.item())\n",
    "          D_losses.append(errD.item())\n",
    "\n",
    "\n",
    "          # Check how the generator is doing by saving G's output on fixed_noise\n",
    "          if (iters % 50 == 0) or ((epoch == max_epochs-1) and (batch_idx == len(training_dataloader)-1)):\n",
    "            with torch.no_grad():\n",
    "                fake = netG(fixed_noise).detach().cpu()\n",
    "                fig, axs = plt.subplots(2)\n",
    "                fig.suptitle('Current fake waves: ')\n",
    "                axs[0].plot(fake[0])\n",
    "                axs[1].plot(fake[4])\n",
    "                plt.show()\n",
    "\n",
    "\n",
    "            # log some values in wandb\n",
    "            wandb.log({\"Loss_D\": errD.item(), \"Loss_G\": errG.item(), \"D(x)\": D_x, \"D(G(z))_coef1\": D_G_z1, \"D(G(z))_coef2\": D_G_z2})\n",
    "\n",
    "            energ_list.append(fake)\n",
    "\n",
    "\n",
    "          iters += 1\n",
    "\n",
    "  plt.clf()\n",
    "  plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "  plt.plot(G_losses,label=\"G\")\n",
    "  plt.plot(D_losses,label=\"D\")\n",
    "  plt.xlabel(\"iterations\")\n",
    "  plt.ylabel(\"Loss\")\n",
    "  plt.legend()\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "  # Mark the run as finished\n",
    "  wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the models that were trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(netD, 'path_to_saved/discriminator_sgan_signature.pt')\n",
    "torch.save(netG, 'path_to_saved/generator_sgan_signature.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gen = torch.load('path_to_saved/generator_sgan_signature.pt')\n",
    "Gen.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create final generator results plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create latent vectors to get results\n",
    "generated_samples = []\n",
    "noise_test = noise_samples(batch_size, nz )\n",
    "with torch.no_grad():\n",
    "    fake = Gen(noise_test).detach().cpu()\n",
    "generated_samples.append(fake)\n",
    "\n",
    "fig, axs = plt.subplots(4)\n",
    "fig.suptitle('Multiple fake and real waves')\n",
    "axs[0].plot(generated_samples[0][0].reshape(-1))\n",
    "axs[1].plot(generated_samples[0][2].reshape(-1))\n",
    "\n",
    "real_batch = next(iter(training_dataloader))\n",
    "real = real_batch.reshape(batch_size, window_size, 1)\n",
    "axs[2].plot(real[0])\n",
    "axs[3].plot(real[1])\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the saved dataset with the 6 hour windows, before the cleaning, to calculate the start times of the devices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pre = pd.read_pickle('path_to_saved/dataport_electric_vehicle_5s.pkl')\n",
    "print(train_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = []\n",
    "for index, data in train_pre.iterrows():\n",
    "  indices.append(next((i for i, x in enumerate(data[0]) if x>0.1), None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the final outputs with post process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the size of the final window to be created\n",
    "final_window = 4319 \n",
    "\n",
    "final = []\n",
    "for fake in generated_samples[0]:\n",
    "  place =random.choice(indices)\n",
    "  result = torch.zeros(1, place)\n",
    "  result = torch.reshape(result, (-1,))\n",
    "  fake[fake < 0] = 0\n",
    "  test = torch.reshape(fake, (-1,))\n",
    "  result = torch.cat((result,test ), 0)\n",
    "  rest  = final_window-window_size-place if final_window-window_size-place>0 else 0\n",
    "  end = torch.zeros(1, rest)\n",
    "  end = torch.reshape(end, (-1,))\n",
    "  fake_final  =  torch.cat((result,end ), 0)\n",
    "  fake_final = fake_final[:final_window]\n",
    "  final.append(fake_final)\n",
    "\n",
    "final = torch.stack(final)\n",
    "\n",
    "# if the size exceeds the max trim the window\n",
    "fig, axs = plt.subplots(8)\n",
    "fig.suptitle('Multiple fake and real waves')\n",
    "axs[0].plot(final[0])\n",
    "axs[1].plot(final[1])\n",
    "axs[2].plot(final[2])\n",
    "axs[3].plot(final[3])\n",
    "axs[4].plot(final[4])\n",
    "axs[5].plot(final[5])\n",
    "axs[6].plot(final[6])\n",
    "axs[7].plot(final[7])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
